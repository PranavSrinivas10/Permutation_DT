{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None): \n",
    "        \n",
    "        # for decision node\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info_gain = info_gain\n",
    "        \n",
    "        # for leaf node\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier():\n",
    "    def __init__(self, min_samples_split=2, max_depth=2):\n",
    "        \n",
    "        # initialize the root of the tree \n",
    "        self.root = None\n",
    "        \n",
    "        # stopping conditions\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def build_tree(self, dataset, curr_depth=0): \n",
    "        \n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "        \n",
    "        # split until stopping conditions are met\n",
    "        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:\n",
    "            # find the best split\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "            # check if information gain is positive\n",
    "            #print(\"INFO_GAIN: \" + str(best_split[\"info_gain\"]))\n",
    "            if best_split[\"info_gain\"]>0:\n",
    "                # recur left\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n",
    "                # recur right\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n",
    "                # return decision node\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
    "                            left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "        \n",
    "        # compute leaf node\n",
    "        leaf_value = self.calculate_leaf_value(Y)\n",
    "        # return leaf node\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    def get_best_split(self, dataset, num_samples, num_features):\n",
    "        \n",
    "        # dictionary to store the best split\n",
    "        best_split = {}\n",
    "        max_info_gain = -float(\"inf\")\n",
    "        \n",
    "        # loop over all the features\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "            # loop over all the feature values present in the data\n",
    "            for threshold in possible_thresholds:\n",
    "                # get current split\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
    "                # check if childs are not null\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    # compute information gain\n",
    "                    curr_info_gain = self.information_gain(y, left_y, right_y, \"etc\")\n",
    "                    # update the best split if needed\n",
    "                    if curr_info_gain>max_info_gain:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"info_gain\"] = curr_info_gain\n",
    "                        max_info_gain = curr_info_gain\n",
    "                        \n",
    "        # return best split\n",
    "        return best_split\n",
    "    \n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        \n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])\n",
    "        return dataset_left, dataset_right\n",
    "    \n",
    "    def information_gain(self, parent, l_child, r_child, mode=\"entropy\"):\n",
    "        \n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        if mode==\"gini\":\n",
    "            gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) + weight_r*self.gini_index(r_child))\n",
    "        elif mode==\"etc\":\n",
    "            gain = self.nsrps(parent,2) - (weight_l*self.nsrps(l_child,2) + weight_r*self.nsrps(r_child,2))\n",
    "        else:\n",
    "            gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))\n",
    "        return gain\n",
    "    \n",
    "    def checkhomogenous(self, arr):\n",
    "        n = len(arr)\n",
    "        if n == 0:\n",
    "            return True\n",
    "        cur = arr[0]\n",
    "        for num in arr:\n",
    "            if num != cur:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def nsrps(self, arr, k):\n",
    "        if self.checkhomogenous(arr):\n",
    "            return 0\n",
    "\n",
    "        n = len(arr)\n",
    "        freq = {}\n",
    "        mxnum = max(arr)\n",
    "        mxfreq = 0\n",
    "        mxtup = ()\n",
    "        for i in range(n - k + 1):\n",
    "            cur = tuple(arr[i:i+k])\n",
    "            if cur in freq:\n",
    "                freq[cur] += 1\n",
    "            else:\n",
    "                freq[cur] = 1\n",
    "            if(freq[cur] > mxfreq):\n",
    "                mxfreq = freq[cur]\n",
    "                mxtup = cur\n",
    "\n",
    "        narr = []\n",
    "        i = 0\n",
    "        while (i < n):\n",
    "            cur = tuple(arr[i:i+k])\n",
    "            if(cur == mxtup):\n",
    "                narr.append(mxnum + 1)\n",
    "                i = i + k\n",
    "            else:\n",
    "                narr.append(arr[i])\n",
    "                i += 1\n",
    "        return 1 + self.nsrps(narr, k)\n",
    "    \n",
    "    def entropy(self, y):\n",
    "        \n",
    "        class_labels = np.unique(y)\n",
    "        entropy = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            entropy += -p_cls * np.log2(p_cls)\n",
    "        return entropy\n",
    "    \n",
    "    def gini_index(self, y):\n",
    "        \n",
    "        class_labels = np.unique(y)\n",
    "        gini = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            gini += p_cls**2\n",
    "        return 1 - gini\n",
    "        \n",
    "    def calculate_leaf_value(self, Y):\n",
    "        \n",
    "        Y = list(Y)\n",
    "        return max(Y, key=Y.count)\n",
    "    \n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        \n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "\n",
    "        else:\n",
    "            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.info_gain)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        \n",
    "        dataset = np.concatenate((X, Y), axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        preditions = [self.make_prediction(x, self.root) for x in X]\n",
    "        return preditions\n",
    "    \n",
    "    def make_prediction(self, x, tree):\n",
    "        \n",
    "        if tree.value!=None: return tree.value\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val<=tree.threshold:\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(x, tree.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1=[1,1,1,2,2,2,4,4]\n",
    "y1=[1,2,3,1,2,3,1,2]\n",
    "x0=[4,4,5,5,5,5]\n",
    "y0=[3,4,3,4,1,2]\n",
    "plt.plot(x0,y0,marker='o',linestyle=\"None\",label=\"Class-0\")\n",
    "plt.plot(x1,y1,marker='x',linestyle=\"None\",label=\"Class-1\")\n",
    "plt.xticks(range(8))\n",
    "plt.yticks(range(8))\n",
    "plt.xlabel(\"f1\")\n",
    "plt.ylabel(\"f2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plotting the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=[1,1,1,2,2,2,4,4,4,4,5,5,5,5]\n",
    "# list of feature 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2=[1,2,3,1,2,3,1,2,3,4,1,2,3,4]\n",
    "# list of feature 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl=[1,1,1,1,1,1,1,1,0,0,0,0,0,0]\n",
    "# list of target values/classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[\"f1\",\"f2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.DataFrame(f1)\n",
    "df2=pd.DataFrame(f2)\n",
    "df3=pd.DataFrame(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf=pd.concat([df1,df2,df3],axis=1,ignore_index=True)\n",
    "# creating a dataframe to represent complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.concat([df1,df2],axis=1,ignore_index=True)\n",
    "labels=df3\n",
    "#creating separtate dataframes for data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename({0: names[0],1: names[1]}, axis=1)\n",
    "labels = labels.rename({0: \"Target\"}, axis=1)\n",
    "# naming columns in respective dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "permorder = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "df=pd.DataFrame(columns=range(tempdf.shape[1]))\n",
    "for i in permorder:\n",
    "    l=list(tempdf.iloc[i-1])\n",
    "    df.loc[len(df.index)]=l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1].to_numpy()\n",
    "Y = df.iloc[:,-1].to_numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoRegressive Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 1000 # number of rows\n",
    "M = 10 # number of columns\n",
    "\n",
    "C1 = [0.9,0.1,0.8,0.2,0.7,0.3,0.6,0.4,0.5,0.15]\n",
    "B1 = C1[::-1]\n",
    "B2 = 0.6\n",
    "\n",
    "cause = [[0 for col in range(M)] for row in range(N)]\n",
    "effect = [[0 for col in range(M)] for row in range(N)]\n",
    "\n",
    "for row in range(N):\n",
    "    for col in range(M):\n",
    "        if row == 0:\n",
    "            cause[row][col] = np.random.normal(0,0.33)\n",
    "        else:\n",
    "            cause[row][col] = C1[col] * cause[row - 1][col] + np.random.normal(0,0.33)\n",
    "\n",
    "for row in range(N):\n",
    "    for col in range(M):\n",
    "        if row == 0:\n",
    "            effect[row][col] = np.random.normal(0,0.33)\n",
    "        else:\n",
    "            effect[row][col] = B1[col] * effect[row - 1][col] + B2 * cause[row - 1][col] + np.random.normal(0,0.33)\n",
    "    \n",
    "# Removing Transient data\n",
    "cause = np.array(cause)\n",
    "cause = cause[100:,:]\n",
    "effect = np.array(effect)\n",
    "effect = effect[100:,:]\n",
    "X_vals = np.zeros((400,M))\n",
    "Y_vals = np.zeros((400,1),dtype='int64')\n",
    "\n",
    "# cause\n",
    "for i in range(200):\n",
    "    X_vals[i] = cause[i]\n",
    "    Y_vals[i] = 0\n",
    "# effect\n",
    "for i in range(200):\n",
    "    X_vals[200+i] = effect[i]\n",
    "    Y_vals[200+i] = 1\n",
    "Y_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in Split  1  :  100.0\n",
      "Accuracy in Split  2  :  100.0\n",
      "Accuracy in Split  3  :  100.0\n",
      "Accuracy in Split  4  :  100.0\n",
      "Accuracy in Split  5  :  44.44444444444444\n",
      "Accuracy in Split  6  :  16.666666666666664\n",
      "Accuracy in Split  7  :  27.77777777777778\n",
      "Accuracy in Split  8  :  22.22222222222222\n",
      "Accuracy in Split  9  :  41.66666666666667\n",
      "Accuracy in Split  10  :  55.55555555555556\n"
     ]
    }
   ],
   "source": [
    "tss = TimeSeriesSplit(n_splits = 10)\n",
    "count = 0\n",
    "for train_index, test_index in tss.split(X_vals):\n",
    "    count+=1\n",
    "    x_train, x_test = X_vals[train_index, :], X_vals[test_index,:]\n",
    "    y_train, y_test = Y_vals[train_index], Y_vals[test_index]\n",
    "    classifier = DecisionTreeClassifier(min_samples_split=2, max_depth=11)\n",
    "    classifier.fit(x_train,y_train)\n",
    "    y_pred = classifier.predict(x_test) \n",
    "    print(\"Accuracy in Split \",count,\" : \", accuracy_score(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y.shape)\n",
    "X_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier(min_samples_split=2, max_depth=20)\n",
    "classifier.fit(X_vals,Y_vals)\n",
    "classifier.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
